# Session Management & Multi-turn Conversations

Complete implementation of session-based conversation context for FinancialAgentia.

## Architecture Overview

### Three Integration Approaches

#### 1. **Session-Based (Recommended for HTTP APIs)**

```python
# In FastAPI endpoint (already implemented in app/main.py)
session_id = request.cookies.get("session_id") or str(uuid.uuid4())
session_store = app.state.session_store  # InMemorySessionStore or RedisSessionStore
orchestrator = app.state.orchestrator

# Run agent with session context
final_answer = await orchestrator.run(
    query=user_query,
    session_id=session_id,
    session_store=session_store,
)
# History automatically saved to session_store[session_id]
```

#### 2. **Explicit History Management**

```python
# For programmatic use
from dexter_py.utils.message_history import MessageHistory

history = MessageHistory(model="gpt-4")
final_answer = await orchestrator.run(
    query="What is Bitcoin?",
    message_history=history,
)
# Access conversation history
for msg in history.get_all():
    print(f"Q: {msg.query}")
    print(f"A: {msg.answer}")
    print(f"Summary: {msg.summary}")
```

#### 3. **Internal Agent History**

```python
# Agent maintains its own history
orchestrator = Orchestrator(model="gpt-4")
await orchestrator.run("First question")
await orchestrator.run("Second question")
# All queries stored in orchestrator.message_history
```

---

## FastAPI Endpoints (app/main.py)

### Create Session

```bash
POST /agent/session

# Response:
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "created"
}
```

Sets `session_id` cookie for subsequent requests.

### Query with Session Context

```bash
POST /agent/query
Content-Type: application/json

{
  "query": "What are the latest crypto price trends?",
  "session_id": "550e8400-e29b-41d4-a716-446655440000"  // optional, uses cookie if not provided
}

# SSE Response (streaming):
data: {"type": "answer", "content": "Recent...", "session_id": "..."}
```

Maintains conversation history across requests. Each query loads session history, runs agent with full context, and saves updated history.

### Get Session History

```bash
GET /agent/history?session_id=550e8400-e29b-41d4-a716-446655440000

# Response:
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "turns": 3,
  "model": "gpt-4",
  "messages": [
    {
      "id": 0,
      "query": "What is Bitcoin?",
      "answer": "Bitcoin is a decentralized...",
      "summary": "Explains Bitcoin fundamentals"
    },
    // ... more turns
  ]
}
```

### Clear Session History

```bash
DELETE /agent/history?session_id=550e8400-e29b-41d4-a716-446655440000

# Response:
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "cleared"
}
```

---

## Session Store Options

### In-Memory (Default for Single Instance)

```python
from dexter_py.utils.session_store import InMemorySessionStore

store = InMemorySessionStore(default_expiry=86400)  # 24 hour expiry

# Get/Set/Delete operations are thread-safe
history = store.get("session-123")
store.set("session-123", history)
store.delete("session-123")
```

**Pros:** Fast, no external dependencies
**Cons:** Doesn't persist across server restarts, limited to single instance

### Redis (Recommended for Production)

```python
# Set environment variable
export REDIS_URL="redis://localhost:6379/0"

# Auto-detected in FastAPI startup
from dexter_py.utils.session_store import get_session_store
store = get_session_store()  # Returns RedisSessionStore if REDIS_URL set
```

**Pros:** Distributed, persistent, horizontal scaling, TTL support
**Cons:** Requires Redis server

---

## LLM-Based Summarization

Summaries are computed when messages are added to history.

### Enable LLM Summarization

```bash
export DEXTER_SUMMARIZE_LLM=true
```

When enabled, each turn's summary is generated by the LLM instead of simple preview:

```python
# With DEXTER_SUMMARIZE_LLM=false (default)
"What is Bitcoin? → Bitcoin is a decentralized peer-to-peer..."

# With DEXTER_SUMMARIZE_LLM=true
"Bitcoin: A decentralized cryptocurrency network using blockchain"
```

LLM summaries are better for:
- Semantic relevance filtering
- Long conversation compression
- Key concept extraction

### Fallback Behavior

If LLM summarization fails, automatically falls back to simple preview:

```python
try:
    summary = llm_summarize(query, answer)
except Exception:
    summary = f"{query[:60]} → {answer[:80]}"  # Fallback
```

---

## Context-Aware Message Selection

For long conversations, the system can select relevant prior messages to reduce token usage.

### Selection Strategies

#### 1. **Embedding-Based Similarity (Most Relevant)**

```bash
export DEXTER_USE_EMBEDDINGS=true
export DEXTER_MAX_CONTEXT_MESSAGES=10
```

Uses `sentence-transformers` to find semantically similar past turns:

```
Current Query: "Bitcoin price predictions"
  ↓
[Embed query]
  ↓
Compare similarity to all past turns' summaries
  ↓
Return top-N most similar messages
```

**Pros:** Contextually relevant
**Cons:** Requires embeddings model (~100MB)

#### 2. **Recency-Based (Fastest)**

```bash
export DEXTER_USE_EMBEDDINGS=false  # default
export DEXTER_MAX_CONTEXT_MESSAGES=10
```

Returns the N most recent messages:

```python
relevant = history._select_by_recency(limit=10)
```

**Pros:** Fast, minimal overhead
**Cons:** May miss relevant older context

---

## Message Class Structure

```python
@dataclass
class Message:
    id: int                # Sequential ID, unique per history
    query: str            # User's question
    answer: str           # Agent's response
    summary: str          # LLM or fallback summary
    
    def __post_init__(self):
        # Validates non-empty strings on creation
```

---

## MessageHistory API

```python
history = MessageHistory(model="gpt-4")

# Add turn (with auto-summary)
history.add_agent_message(query, answer)

# Add turn with custom summary
history.add_agent_message(query, answer, summary="Custom summary")

# Access messages
all_messages = history.get_all()  # List[Message]
last_message = history.last()      # Message | None
message = history.get_by_id(0)     # Message | None

# Select relevant
relevant = await history.select_relevant_messages(current_query)

# Format for prompts
text = history.format_for_planning()      # Conversation context for planning
text = history.format_for_context()       # Full history for system prompt

# Utilities
count = len(history)              # Number of turns
has_msgs = history.has_messages() # bool
history.clear()                   # Clear all (resets IDs to 0)
```

---

## Configuration

### Environment Variables

```bash
# Session Store
REDIS_URL=redis://localhost:6379/0        # If set, uses Redis; else in-memory

# Summarization
DEXTER_SUMMARIZE_LLM=true|false           # Enable LLM-based summaries (default: false)

# Message Selection
DEXTER_USE_EMBEDDINGS=true|false          # Enable embedding similarity (default: false)
DEXTER_MAX_CONTEXT_MESSAGES=10            # Max messages to include in context (default: 10)

# FastAPI Security
JWT_SECRET=your-secret-key                # Enable JWT auth
BACKEND_API_KEY=api-key-value             # Enable API key auth
SECURE_COOKIES=true|false                 # Secure flag for session cookies (default: false)
```

---

## Example: Multi-turn Financial Query

```python
# Client code
session_id = str(uuid.uuid4())

# Turn 1: General question
response1 = await client.post(
    "/agent/query",
    json={"query": "What is the current market cap of Bitcoin?", "session_id": session_id}
)
# Agent response: "As of [date], Bitcoin's market cap is..."

# Turn 2: Follow-up (context-aware)
response2 = await client.post(
    "/agent/query",
    json={"query": "How does this compare to Ethereum?", "session_id": session_id}
)
# Agent response: "Compared to Bitcoin's [market cap], Ethereum's is..."
# ^ Uses Turn 1 context automatically!

# Turn 3: Trend analysis
response3 = await client.post(
    "/agent/query",
    json={"query": "What factors typically drive these values?", "session_id": session_id}
)
# Agent response: "Based on the trends we've discussed..."
# ^ Uses both Turn 1 and 2 context!

# View full conversation
history = await client.get(
    "/agent/history",
    params={"session_id": session_id}
)
# Returns all 3 turns with summaries
```

---

## Thread Safety & Concurrency

### InMemorySessionStore
- Uses `threading.RLock` for thread-safe access
- Safe for multi-threaded FastAPI servers

### RedisSessionStore
- Redis itself handles concurrency
- All operations atomic at Redis level

### MessageHistory
- Immutable messages (no mutation after creation)
- Safe to access concurrently
- Not thread-locked (doesn't need to be)

---

## Performance Characteristics

| Operation | Complexity | Time |
|-----------|-----------|------|
| `add_agent_message()` | O(1) | ~10-100ms (with LLM summarization) |
| `select_relevant_messages()` | O(n) | O(n) simple, O(n·log n) with embeddings |
| Session store get/set | O(1) | ~1ms (in-memory), ~5-10ms (Redis) |
| History formatting | O(n) | ~1-5ms for 100 messages |

---

## Migration from Internal History

If you have code using `orchestrator.message_history` directly:

**Before:**
```python
orchestrator = Orchestrator(model="gpt-4")
await orchestrator.run("Query 1")
await orchestrator.run("Query 2")
# Uses internal orchestrator.message_history
```

**After (with sessions):**
```python
orchestrator = Orchestrator(model="gpt-4")
session_store = InMemorySessionStore()
session_id = str(uuid.uuid4())

await orchestrator.run("Query 1", session_id=session_id, session_store=session_store)
await orchestrator.run("Query 2", session_id=session_id, session_store=session_store)
# Uses session_store[session_id]
```

Both patterns work—choose based on your deployment model.
